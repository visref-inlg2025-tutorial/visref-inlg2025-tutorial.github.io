<!DOCTYPE html>
<html lang="en" class=" usyvdii idc0_336"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link href="style.css" rel="stylesheet">
    <title>INLG 2025 tutorial - Visual and Task Context for Multi-Modal Reference</title>
  </head>
  <body class="vsc-initialized" style="">
    <div class="container bg-light">
        <br>
        <h2>Visual and Task Context for Multi-Modal Reference</h2>
        <p class="lead">A <a href="https://2025.inlgmeeting.org">INLG 2025</a> tutorial by <a href="X">Nikolai Ilinykh</a> (University of Gothenburg, Sweden), <a href="X">Simon Dobnik</a> (University of Gothenburg, Sweden), <a href="X">X</a> (X), <a href="X">X</a> (X) and <a href="X">X</a> (X).</p>

        <p class="lead">Slides:<br>
        &#8226; Part 1: <a href=X>What is to ``refer''?</a> <br>
        &#8226; Part 2: <a href=X>Manifestation of reference in tasks and contexts</a><br>
        &#8226; Part 3: <a href=X>Evaluation of reference</a><br>
        &#8226; Hands-on session: <a href=X>Code</a>
        
        </p>

        <p class="lead">

            Our tutorial aims to give researchers a single, context-aware toolkit for evaluating how vision-and-language models produce reference.
            Multi-modal systems are advancing faster than their evaluation benchmarks.
            Without consistent, context-sensitive tests, progress is hard to measure.
            This tutorial is thus timely as it unifies the latest methods, mapping the key visual and task factors important for modelling referring, and packaging them into a ready-to-use resource.
        </p>

        <p class="lead">

            Because context -- both the visual alternatives available and the task of referring to a target -- shapes human referring expressions <a href="#bib-frank2012">(Frank et al., 2012)</a>, vision-and-language text generation models must properly understand context of reference.
            Evaluating reference is challenging precisely because it is so context-dependent, spanning the visual scene and the broader task setting.
            
           
        </p>
        
          <p class="lead">Click <a href="nikolai.ilinykh@gu.se">here to
          contact us</a>.</p>

        <br>

    </div>

    <!-- Add bibliography section before </body> -->
<div class="container">
  <h3>References</h3>
  <ul>
    <li id="bib-frank2012">Frank 2012. "Predicting Pragmatic Reasoning in Language Games" <i>Journal Name</i>.</li>
  </ul>
</div>
  </body>
</html>